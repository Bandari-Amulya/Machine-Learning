{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "SenxHpKnhJra",
        "outputId": "bbe17aeb-1f1b-423d-e638-c84c8ca62560"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m266.2/275.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'listings.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1138340411.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m     \u001b[0;31m# Load and preprocess data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m     \u001b[0mX_train_pre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_pre\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_num\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;31m# A1: Feature correlation analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1138340411.py\u001b[0m in \u001b[0;36mload_and_preprocess_data\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_and_preprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'listings.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Handle missing or non-numeric prices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'listings.csv'"
          ]
        }
      ],
      "source": [
        "# Install required packages if not already installed\n",
        "!pip install lime shap -q\n",
        "\n",
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SequentialFeatureSelector\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import lime\n",
        "import lime.lime_tabular\n",
        "import shap\n",
        "\n",
        "# Function to load and preprocess the data\n",
        "# Returns preprocessed X_train, X_test, y_train, y_test, preprocessor, feature_names, and numerical df for correlation\n",
        "def load_and_preprocess_data(file_path='listings.csv'):\n",
        "    # Load the dataset\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # Handle missing or non-numeric prices\n",
        "    df['price'] = pd.to_numeric(df['price'], errors='coerce')\n",
        "    df = df.dropna(subset=['price'])\n",
        "\n",
        "    # Fill missing values in other columns\n",
        "    df['reviews_per_month'] = df['reviews_per_month'].fillna(0)\n",
        "    df['number_of_reviews_ltm'] = df['number_of_reviews_ltm'].fillna(0)\n",
        "    df['availability_365'] = df['availability_365'].fillna(0)\n",
        "    df = df.fillna(0)  # General fill for any remaining\n",
        "\n",
        "    # Select features\n",
        "    numerical_features = ['latitude', 'longitude', 'minimum_nights', 'number_of_reviews',\n",
        "                          'reviews_per_month', 'calculated_host_listings_count',\n",
        "                          'availability_365', 'number_of_reviews_ltm']\n",
        "    categorical_features = ['neighbourhood_group', 'neighbourhood', 'room_type']\n",
        "\n",
        "    # Data for correlation (numerical + target)\n",
        "    df_num = df[numerical_features + ['price']]\n",
        "\n",
        "    # Features and target\n",
        "    X = df[numerical_features + categorical_features]\n",
        "    y = df['price']\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Preprocessor\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', StandardScaler(), numerical_features),\n",
        "            ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_features)\n",
        "        ])\n",
        "\n",
        "    # Fit preprocessor\n",
        "    X_train_pre = preprocessor.fit_transform(X_train)\n",
        "    X_test_pre = preprocessor.transform(X_test)\n",
        "\n",
        "    # Get feature names after preprocessing\n",
        "    feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "    return X_train_pre, X_test_pre, y_train, y_test, preprocessor, feature_names, df_num\n",
        "\n",
        "# Function for A1: Compute correlation matrix\n",
        "# Returns the correlation matrix\n",
        "def compute_correlation(df_num):\n",
        "    corr_matrix = df_num.corr()\n",
        "    return corr_matrix\n",
        "\n",
        "# Function for A2/A3: Perform PCA with specified variance threshold\n",
        "# Returns reduced X_train, reduced X_test, number of components, cumulative explained variance\n",
        "def perform_pca(X_train_pre, X_test_pre, variance_threshold=0.99):\n",
        "    pca = PCA()\n",
        "    pca.fit(X_train_pre)\n",
        "    cum_var = np.cumsum(pca.explained_variance_ratio_)\n",
        "    n_comp = np.where(cum_var >= variance_threshold)[0][0] + 1\n",
        "    pca = PCA(n_components=n_comp)\n",
        "    X_train_reduced = pca.fit_transform(X_train_pre)\n",
        "    X_test_reduced = pca.transform(X_test_pre)\n",
        "    return X_train_reduced, X_test_reduced, n_comp, cum_var\n",
        "\n",
        "# Function to run regression models and evaluate\n",
        "# Returns a dictionary of model results with MSE and R2\n",
        "def run_regression_models(X_train, y_train, X_test, y_test):\n",
        "    models = {\n",
        "        'LinearRegression': LinearRegression(),\n",
        "        'RandomForestRegressor': RandomForestRegressor(random_state=42, n_jobs=-1)\n",
        "    }\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "        results[name] = {'MSE': mse, 'R2': r2}\n",
        "    return results, models['RandomForestRegressor']  # Return RF model for explanations\n",
        "\n",
        "# Function for A4: Perform sequential feature selection\n",
        "# Returns reduced X_train, reduced X_test, selected feature mask\n",
        "def perform_sequential_feature_selection(X_train_pre, y_train, X_test_pre, direction='forward'):\n",
        "    estimator = LinearRegression()  # Use LR for selector to avoid slowness with RF\n",
        "    sfs = SequentialFeatureSelector(estimator, n_features_to_select='auto', direction=direction, scoring='r2', n_jobs=-1)\n",
        "    sfs.fit(X_train_pre, y_train)\n",
        "    X_train_sfs = sfs.transform(X_train_pre)\n",
        "    X_test_sfs = sfs.transform(X_test_pre)\n",
        "    return X_train_sfs, X_test_sfs, sfs.support_\n",
        "\n",
        "# Function to get LIME explainer and explanation for a specific instance\n",
        "# Returns the LIME explanation object\n",
        "def get_lime_explanation(model, X_train_pre, X_test_pre, feature_names, instance_idx=0):\n",
        "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
        "        X_train_pre,\n",
        "        feature_names=feature_names,\n",
        "        mode=\"regression\"\n",
        "    )\n",
        "    explanation = explainer.explain_instance(X_test_pre[instance_idx], model.predict)\n",
        "    return explanation\n",
        "\n",
        "# Function to get SHAP values\n",
        "# Returns SHAP values for the test set\n",
        "def get_shap_values(model, X_test_pre):\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    shap_values = explainer.shap_values(X_test_pre)\n",
        "    return shap_values\n",
        "\n",
        "# Main program\n",
        "if __name__ == \"__main__\":\n",
        "    # Load and preprocess data\n",
        "    X_train_pre, X_test_pre, y_train, y_test, preprocessor, feature_names, df_num = load_and_preprocess_data()\n",
        "\n",
        "    # A1: Feature correlation analysis\n",
        "    corr_matrix = compute_correlation(df_num)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
        "    plt.title(\"Feature Correlation Heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "    # Baseline model performance (without reduction)\n",
        "    baseline_results, _ = run_regression_models(X_train_pre, y_train, X_test_pre, y_test)\n",
        "    print(\"Baseline Model Performance (No Reduction):\")\n",
        "    for model, metrics in baseline_results.items():\n",
        "        print(f\"{model} - MSE: {metrics['MSE']:.2f}, R2: {metrics['R2']:.2f}\")\n",
        "\n",
        "    # A2: PCA with 99% variance\n",
        "    X_train_pca99, X_test_pca99, n_comp99, cum_var99 = perform_pca(X_train_pre, X_test_pre, 0.99)\n",
        "    print(f\"\\nA2: PCA with 99% variance retained {n_comp99} components.\")\n",
        "    pca99_results, _ = run_regression_models(X_train_pca99, y_train, X_test_pca99, y_test)\n",
        "    print(\"A2 Model Performance (PCA 99%):\")\n",
        "    for model, metrics in pca99_results.items():\n",
        "        print(f\"{model} - MSE: {metrics['MSE']:.2f}, R2: {metrics['R2']:.2f}\")\n",
        "\n",
        "    # A3: PCA with 95% variance\n",
        "    X_train_pca95, X_test_pca95, n_comp95, cum_var95 = perform_pca(X_train_pre, X_test_pre, 0.95)\n",
        "    print(f\"\\nA3: PCA with 95% variance retained {n_comp95} components.\")\n",
        "    pca95_results, _ = run_regression_models(X_train_pca95, y_train, X_test_pca95, y_test)\n",
        "    print(\"A3 Model Performance (PCA 95%):\")\n",
        "    for model, metrics in pca95_results.items():\n",
        "        print(f\"{model} - MSE: {metrics['MSE']:.2f}, R2: {metrics['R2']:.2f}\")\n",
        "\n",
        "    # A4: Sequential Feature Selection\n",
        "    X_train_sfs, X_test_sfs, sfs_mask = perform_sequential_feature_selection(X_train_pre, y_train, X_test_pre)\n",
        "    num_selected = sum(sfs_mask)\n",
        "    print(f\"\\nA4: Sequential Feature Selection retained {num_selected} features.\")\n",
        "    sfs_results, _ = run_regression_models(X_train_sfs, y_train, X_test_sfs, y_test)\n",
        "    print(\"A4 Model Performance (SFS):\")\n",
        "    for model, metrics in sfs_results.items():\n",
        "        print(f\"{model} - MSE: {metrics['MSE']:.2f}, R2: {metrics['R2']:.2f}\")\n",
        "\n",
        "    # Comparison\n",
        "    print(\"\\nComparison of Results:\")\n",
        "    print(\"Baseline vs PCA99 vs PCA95 vs SFS\")\n",
        "    for model in baseline_results.keys():\n",
        "        print(f\"{model}:\")\n",
        "        print(f\"  Baseline - MSE: {baseline_results[model]['MSE']:.2f}, R2: {baseline_results[model]['R2']:.2f}\")\n",
        "        print(f\"  PCA99 - MSE: {pca99_results[model]['MSE']:.2f}, R2: {pca99_results[model]['R2']:.2f}\")\n",
        "        print(f\"  PCA95 - MSE: {pca95_results[model]['MSE']:.2f}, R2: {pca95_results[model]['R2']:.2f}\")\n",
        "        print(f\"  SFS - MSE: {sfs_results[model]['MSE']:.2f}, R2: {sfs_results[model]['R2']:.2f}\")\n",
        "\n",
        "    # A5: LIME and SHAP explanations (using RandomForest from baseline)\n",
        "    _, rf_model = run_regression_models(X_train_pre, y_train, X_test_pre, y_test)  # Refit for explanations\n",
        "\n",
        "    # LIME\n",
        "    print(\"\\nA5: LIME Explanation for first test instance\")\n",
        "    lime_exp = get_lime_explanation(rf_model, X_train_pre, X_test_pre, feature_names, instance_idx=0)\n",
        "    lime_exp.show_in_notebook(show_table=True)\n",
        "\n",
        "    # SHAP\n",
        "    print(\"A5: SHAP Summary Plot\")\n",
        "    shap_values = get_shap_values(rf_model, X_test_pre)\n",
        "    shap.summary_plot(shap_values, X_test_pre, feature_names=feature_names, show=True)"
      ]
    }
  ]
}